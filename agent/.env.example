# YOU+ Future Self Agent Environment Variables
# ==============================================

# Required for Cartesia deployment (auto-injected by cartesia deploy)
# CARTESIA_API_KEY=your_cartesia_api_key

# Required for LLM reasoning (production)
GEMINI_API_KEY=your_gemini_api_key

# Required for user context fetching
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_KEY=your_supabase_service_key

# Supermemory - Dynamic user profiles and memory
# Get API key at: https://console.supermemory.ai
SUPERMEMORY_API_KEY=sm_your_api_key_here

# Backend URL for call result reporting
BACKEND_URL=https://youplus-backend.workers.dev

# Optional: Override the default model
# MODEL_ID=gemini-2.5-flash

# ==============================================
# TESTING LLM OPTIONS
# ==============================================

# Option 1: Ollama (local, free, no API key needed)
# Just make sure Ollama is running: ollama serve
# And pull a model: ollama pull qwen2.5:14b
OLLAMA_URL=http://localhost:11434

# Option 2: Groq (cloud, free tier, better quality)
# Get free API key at: https://console.groq.com
# GROQ_API_KEY=your_groq_api_key

# ==============================================
# TESTING GUIDE
# ==============================================
#
# 1. FIRST: Run the migration to create the call_memory table
#    psql $DATABASE_URL < ../migrations/add_call_memory.sql
#
# 2. LOCAL TESTING with different LLMs:
#
#    # Ollama (default - local, free)
#    ollama serve                          # Start Ollama
#    ollama pull qwen2.5:14b               # Pull model (first time only)
#    uv run python test_local.py --llm ollama
#
#    # Groq (cloud, free, better quality)
#    uv run python test_local.py --llm groq
#
#    # Gemini (original)
#    uv run python test_local.py --llm gemini
#
#    # Custom model
#    uv run python test_local.py --llm ollama --model qwen2.5:32b
#    uv run python test_local.py --llm groq --model llama-3.3-70b-versatile
#
# 3. TEST SCENARIOS:
#
#    # Basic test - auto-selects call type and mood
#    uv run python test_local.py
#
#    # Test specific call type
#    uv run python test_local.py --call-type milestone
#    uv run python test_local.py --call-type challenge
#    uv run python test_local.py --call-type story
#
#    # Test specific mood
#    uv run python test_local.py --mood cold_intense
#    uv run python test_local.py --mood dark_prophetic
#
#    # Test milestone reveals (override streak)
#    uv run python test_local.py --streak 7 --call-type milestone
#    uv run python test_local.py --streak 30 --call-type milestone
#
#    # Test broken promise scenario (cold/dark mood)
#    uv run python test_local.py --promise-kept false
#
#    # Just show prompts, no interactive mode
#    uv run python test_local.py --no-interactive
#
# 4. REAL CALL TESTING:
#
#    cartesia auth login
#    cartesia deploy
#    cartesia call +1XXXXXXXXXX --metadata '{"user_id": "your-uuid"}'
#
# ==============================================
# LLM QUALITY COMPARISON
# ==============================================
#
# | Model                    | Quality | Speed | Cost  |
# |--------------------------|---------|-------|-------|
# | Gemini 2.5 Flash         | ⭐⭐⭐⭐   | Fast  | $     |
# | Groq Llama 3.1 70B       | ⭐⭐⭐⭐   | Fast  | Free  |
# | Ollama Qwen 2.5 32B      | ⭐⭐⭐    | Slow  | Free  |
# | Ollama Qwen 2.5 14B      | ⭐⭐⭐    | Med   | Free  |
# | Ollama Qwen 2.5 7B       | ⭐⭐     | Fast  | Free  |
#
# Recommendation:
# - Testing flow/system: Ollama Qwen 2.5 14B (free, good enough)
# - Testing quality: Groq Llama 3.1 70B (free, much better)
# - Production: Gemini 2.5 Flash (cost-effective, good quality)
#

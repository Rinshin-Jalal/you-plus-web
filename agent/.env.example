# YOU+ Future Self Agent Environment Variables
# ==============================================

# Required for Cartesia deployment (auto-injected by cartesia deploy)
# CARTESIA_API_KEY=your_cartesia_api_key

# ==============================================
# LLM CONFIGURATION (OpenAI-compatible API)
# ==============================================
# The agent uses an OpenAI-compatible API endpoint.
# Configure these for AWS Bedrock, OpenAI, Groq, or any compatible provider.

# Required: API key for authentication
LLM_API_KEY=your_api_key_here

# Required: Base URL for the API endpoint
# Examples:
#   OpenAI: https://api.openai.com/v1
#   AWS Bedrock: https://bedrock-runtime.us-east-1.amazonaws.com
#   Groq: https://api.groq.com/openai/v1
LLM_BASE_URL=https://api.openai.com/v1

# Required: Model ID to use
# Examples:
#   OpenAI: gpt-4o-mini, gpt-4o
#   AWS Bedrock: anthropic.claude-3-sonnet, meta.llama3-70b-instruct-v1
#   Groq: llama-3.1-70b-versatile
LLM_MODEL=gpt-4o-mini

# ==============================================
# OTHER SERVICES
# ==============================================

# Required for user context fetching
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_KEY=your_supabase_service_key

# Supermemory - Dynamic user profiles and memory
# Get API key at: https://console.supermemory.ai
SUPERMEMORY_API_KEY=sm_your_api_key_here

# Backend URL for call result reporting
BACKEND_URL=https://youplus-backend.workers.dev

# Optional: Override Gemini model (for main speaking agent)
# GEMINI_API_KEY=your_gemini_api_key
# MODEL_ID=gemini-2.5-flash

# ==============================================
# LOCAL TESTING OPTIONS
# ==============================================

# Option 1: Ollama (local, free, no API key needed)
# Just make sure Ollama is running: ollama serve
# And pull a model: ollama pull qwen2.5:14b
OLLAMA_URL=http://localhost:11434

# ==============================================
# TESTING GUIDE
# ==============================================
#
# 1. FIRST: Run the migration to create the call_memory table
#    psql $DATABASE_URL < ../migrations/add_call_memory.sql
#
# 2. LOCAL TESTING:
#
#    # Using Ollama (local, free)
#    ollama serve                          # Start Ollama
#    ollama pull qwen2.5:14b               # Pull model (first time only)
#    export LLM_BASE_URL=http://localhost:11434/v1
#    export LLM_MODEL=qwen2.5:14b
#    export LLM_API_KEY=ollama             # Ollama doesn't need a real key
#    uv run python test_local.py
#
#    # Using AWS Bedrock (production)
#    export LLM_BASE_URL=https://bedrock-runtime.us-east-1.amazonaws.com
#    export LLM_MODEL=your-model-id
#    export LLM_API_KEY=your-api-key
#    uv run python test_local.py
#
# 3. TEST SCENARIOS:
#
#    # Basic test - auto-selects call type and mood
#    uv run python test_local.py
#
#    # Test specific call type
#    uv run python test_local.py --call-type milestone
#    uv run python test_local.py --call-type challenge
#    uv run python test_local.py --call-type story
#
#    # Test specific mood
#    uv run python test_local.py --mood cold_intense
#    uv run python test_local.py --mood dark_prophetic
#
#    # Test milestone reveals (override streak)
#    uv run python test_local.py --streak 7 --call-type milestone
#    uv run python test_local.py --streak 30 --call-type milestone
#
#    # Test broken promise scenario (cold/dark mood)
#    uv run python test_local.py --promise-kept false
#
#    # Just show prompts, no interactive mode
#    uv run python test_local.py --no-interactive
#
# 4. REAL CALL TESTING:
#
#    cartesia auth login
#    cartesia deploy
#    cartesia call +1XXXXXXXXXX --metadata '{"user_id": "your-uuid"}'
#
